{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ef258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f979727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ruta al CSV (usa test o train según corresponda)\n",
    "csv_path_train = \"/workspace/sign_mnist_train.csv\"\n",
    "csv_path_test = \"/workspace/sign_mnist_test.csv\"\n",
    "df_train = pd.read_csv(csv_path_train)\n",
    "df_test = pd.read_csv(csv_path_test)\n",
    "\n",
    "#Separar etiquetas y píxeles\n",
    "labels = df_test['label'].values.astype(np.int64)\n",
    "pixels = df_test.drop(columns='label').to_numpy().astype(np.float32)\n",
    "labels = df_train['label'].values.astype(np.int64)\n",
    "pixels = df_train.drop(columns='label').to_numpy().astype(np.float32)\n",
    "#Reformar a imágenes 28x28\n",
    "image_test = pixels.reshape(-1, 28, 28)\n",
    "image_train = pixels.reshape(-1, 28, 28)\n",
    "\n",
    "print(\"Imágen test:\", image_test.shape)\n",
    "print(\"Imágen train:\", image_train.shape)\n",
    "print(\"Labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefda4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "img_np = image_test[idx]\n",
    "label = labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformación base: de numpy a tensor (1 canal)\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_tensor = to_tensor(img_np)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea525f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Rotación\n",
    "rot_transform = transforms.RandomRotation(degrees=30)\n",
    "\n",
    "# 2 - Traslación (usando affine)\n",
    "translate_transform = transforms.RandomAffine(\n",
    "    degrees=0, translate=(0.2, 0.2)  # hasta 20% en cada eje\n",
    ")\n",
    "\n",
    "# 3 - Escalado (zoom)\n",
    "scale_transform = transforms.RandomAffine(\n",
    "    degrees=0, scale=(0.8, 1.2)\n",
    ")\n",
    "\n",
    "# 4 - Inversión de colores (para tensor)\n",
    "invert_transform = transforms.RandomInvert(p=1.0)\n",
    "\n",
    "# 5 - Adición de ruido (definimos una función propia)\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
    "        return torch.clamp(tensor + noise, 0.0, 1.0)\n",
    "\n",
    "noise_transform = AddGaussianNoise(mean=0.0, std=0.2)\n",
    "\n",
    "# 6) Recorte aleatorio\n",
    "# Para recorte en 28x28, ampliamos a 32x32 y recortamos\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop((28, 28))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e48ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aplicamos transformaciones y variables para visualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rot   = rot_transform(img_tensor)\n",
    "img_trans = translate_transform(img_tensor)\n",
    "img_scale = scale_transform(img_tensor)\n",
    "img_inv   = invert_transform(img_tensor)\n",
    "img_noise = noise_transform(img_tensor)\n",
    "img_crop  = crop_transform(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gray_image(tensor, title=\"\"):\n",
    "    img = tensor.squeeze(0).numpy()\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 4, 1)\n",
    "show_gray_image(img_tensor, title=f\"Original (label={label})\")\n",
    "\n",
    "plt.subplot(2, 4, 2)\n",
    "show_gray_image(img_rot, \"Rotación\")\n",
    "\n",
    "plt.subplot(2, 4, 3)\n",
    "show_gray_image(img_trans, \"Traslación\")\n",
    "\n",
    "plt.subplot(2, 4, 4)\n",
    "show_gray_image(img_scale, \"Escalado\")\n",
    "\n",
    "plt.subplot(2, 4, 5)\n",
    "show_gray_image(img_inv, \"Inversión\")\n",
    "\n",
    "plt.subplot(2, 4, 6)\n",
    "show_gray_image(img_noise, \"Ruido\")\n",
    "\n",
    "plt.subplot(2, 4, 7)\n",
    "show_gray_image(img_crop, \"Random Crop\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignMNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = img.astype(np.float32) / 255.0 \n",
    "        img = torch.from_numpy(img).unsqueeze(0) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "#Normalización para CNN\n",
    "cnn_transform = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "full_dataset = SignMNISTDataset(images, labels, transform=cnn_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50797be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae459cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # Ejemplo base: kernel_size=3, padding=1, stride=1, pool 2x2\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        fc_input = 64 * 7 * 7\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#Número de clases en Sign Language MNIST (generalmente 24 o 25, según versión)\n",
    "num_classes = len(np.unique(labels))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Entrenamiento ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / train_total\n",
    "    train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "    #Validación\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * x_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "    print(f\"Época [{epoch+1}/{num_epochs}] \"\n",
    "          f\"- Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "          f\"- Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
